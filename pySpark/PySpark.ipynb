{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install Apache Spark 3.2.0\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set up environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"SalesAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "input_file_path = \"/content/sales_data.csv\"\n",
        "sales_data = spark.read.csv(input_file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Perform data cleaning: handle missing values and remove duplicates\n",
        "cleaned_sales_data = sales_data.dropDuplicates().na.drop()\n",
        "\n",
        "# Calculate the total sales amount for each product\n",
        "total_sales_per_product = (\n",
        "    cleaned_sales_data\n",
        "    .groupBy(\"Product\")\n",
        "    .agg(sum(\"SalesAmount\").alias(\"TotalSalesAmount\"))\n",
        ")\n",
        "\n",
        "# Output the results to a new CSV file\n",
        "output_file_path = \"/content/output_file.csv\"\n",
        "total_sales_per_product.write.csv(output_file_path, header=True)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "7oF14kBCJ4Be"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}